{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2338763c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: groq in ./venv/lib/python3.12/site-packages (1.0.0)\n",
      "Requirement already satisfied: langchain in ./venv/lib/python3.12/site-packages (1.2.0)\n",
      "Requirement already satisfied: langchain-community in ./venv/lib/python3.12/site-packages (0.4.1)\n",
      "Requirement already satisfied: sentence-transformers in ./venv/lib/python3.12/site-packages (5.2.0)\n",
      "Requirement already satisfied: chromadb in ./venv/lib/python3.12/site-packages (1.4.0)\n",
      "Requirement already satisfied: pypdf in ./venv/lib/python3.12/site-packages (6.5.0)\n",
      "Requirement already satisfied: python-dotenv in ./venv/lib/python3.12/site-packages (1.2.1)\n",
      "Requirement already satisfied: ipykernel in ./venv/lib/python3.12/site-packages (7.1.0)\n",
      "Requirement already satisfied: google-genai in ./venv/lib/python3.12/site-packages (1.56.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in ./venv/lib/python3.12/site-packages (from groq) (4.12.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in ./venv/lib/python3.12/site-packages (from groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./venv/lib/python3.12/site-packages (from groq) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in ./venv/lib/python3.12/site-packages (from groq) (2.12.5)\n",
      "Requirement already satisfied: sniffio in ./venv/lib/python3.12/site-packages (from groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in ./venv/lib/python3.12/site-packages (from groq) (4.15.0)\n",
      "Requirement already satisfied: langchain-core<2.0.0,>=1.2.1 in ./venv/lib/python3.12/site-packages (from langchain) (1.2.5)\n",
      "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in ./venv/lib/python3.12/site-packages (from langchain) (1.0.5)\n",
      "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in ./venv/lib/python3.12/site-packages (from langchain-community) (1.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in ./venv/lib/python3.12/site-packages (from langchain-community) (2.0.45)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.32.5 in ./venv/lib/python3.12/site-packages (from langchain-community) (2.32.5)\n",
      "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in ./venv/lib/python3.12/site-packages (from langchain-community) (6.0.3)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./venv/lib/python3.12/site-packages (from langchain-community) (3.13.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in ./venv/lib/python3.12/site-packages (from langchain-community) (9.1.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in ./venv/lib/python3.12/site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in ./venv/lib/python3.12/site-packages (from langchain-community) (2.12.0)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in ./venv/lib/python3.12/site-packages (from langchain-community) (0.5.1)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in ./venv/lib/python3.12/site-packages (from langchain-community) (0.4.3)\n",
      "Requirement already satisfied: numpy>=1.26.2 in ./venv/lib/python3.12/site-packages (from langchain-community) (2.4.0)\n",
      "Requirement already satisfied: transformers<6.0.0,>=4.41.0 in ./venv/lib/python3.12/site-packages (from sentence-transformers) (4.57.3)\n",
      "Requirement already satisfied: tqdm in ./venv/lib/python3.12/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in ./venv/lib/python3.12/site-packages (from sentence-transformers) (2.9.1)\n",
      "Requirement already satisfied: scikit-learn in ./venv/lib/python3.12/site-packages (from sentence-transformers) (1.8.0)\n",
      "Requirement already satisfied: scipy in ./venv/lib/python3.12/site-packages (from sentence-transformers) (1.16.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in ./venv/lib/python3.12/site-packages (from sentence-transformers) (0.36.0)\n",
      "Requirement already satisfied: build>=1.0.3 in ./venv/lib/python3.12/site-packages (from chromadb) (1.3.0)\n",
      "Requirement already satisfied: pybase64>=1.4.1 in ./venv/lib/python3.12/site-packages (from chromadb) (1.4.3)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in ./venv/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.40.0)\n",
      "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in ./venv/lib/python3.12/site-packages (from chromadb) (5.4.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in ./venv/lib/python3.12/site-packages (from chromadb) (1.23.2)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in ./venv/lib/python3.12/site-packages (from chromadb) (1.39.1)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in ./venv/lib/python3.12/site-packages (from chromadb) (1.39.1)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in ./venv/lib/python3.12/site-packages (from chromadb) (1.39.1)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in ./venv/lib/python3.12/site-packages (from chromadb) (0.22.1)\n",
      "Requirement already satisfied: pypika>=0.48.9 in ./venv/lib/python3.12/site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: overrides>=7.3.1 in ./venv/lib/python3.12/site-packages (from chromadb) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in ./venv/lib/python3.12/site-packages (from chromadb) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in ./venv/lib/python3.12/site-packages (from chromadb) (1.76.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in ./venv/lib/python3.12/site-packages (from chromadb) (5.0.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in ./venv/lib/python3.12/site-packages (from chromadb) (0.21.0)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in ./venv/lib/python3.12/site-packages (from chromadb) (34.1.0)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in ./venv/lib/python3.12/site-packages (from chromadb) (5.2.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in ./venv/lib/python3.12/site-packages (from chromadb) (3.11.5)\n",
      "Requirement already satisfied: rich>=10.11.0 in ./venv/lib/python3.12/site-packages (from chromadb) (14.2.0)\n",
      "Requirement already satisfied: jsonschema>=4.19.0 in ./venv/lib/python3.12/site-packages (from chromadb) (4.25.1)\n",
      "Requirement already satisfied: comm>=0.1.1 in ./venv/lib/python3.12/site-packages (from ipykernel) (0.2.3)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in ./venv/lib/python3.12/site-packages (from ipykernel) (1.8.19)\n",
      "Requirement already satisfied: ipython>=7.23.1 in ./venv/lib/python3.12/site-packages (from ipykernel) (9.8.0)\n",
      "Requirement already satisfied: jupyter-client>=8.0.0 in ./venv/lib/python3.12/site-packages (from ipykernel) (8.7.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in ./venv/lib/python3.12/site-packages (from ipykernel) (5.9.1)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in ./venv/lib/python3.12/site-packages (from ipykernel) (0.2.1)\n",
      "Requirement already satisfied: nest-asyncio>=1.4 in ./venv/lib/python3.12/site-packages (from ipykernel) (1.6.0)\n",
      "Requirement already satisfied: packaging>=22 in ./venv/lib/python3.12/site-packages (from ipykernel) (25.0)\n",
      "Requirement already satisfied: psutil>=5.7 in ./venv/lib/python3.12/site-packages (from ipykernel) (7.2.1)\n",
      "Requirement already satisfied: pyzmq>=25 in ./venv/lib/python3.12/site-packages (from ipykernel) (27.1.0)\n",
      "Requirement already satisfied: tornado>=6.2 in ./venv/lib/python3.12/site-packages (from ipykernel) (6.5.4)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in ./venv/lib/python3.12/site-packages (from ipykernel) (5.14.3)\n",
      "Requirement already satisfied: google-auth<3.0.0,>=2.45.0 in ./venv/lib/python3.12/site-packages (from google-auth[requests]<3.0.0,>=2.45.0->google-genai) (2.45.0)\n",
      "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in ./venv/lib/python3.12/site-packages (from google-genai) (15.0.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in ./venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in ./venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./venv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
      "Requirement already satisfied: idna>=2.8 in ./venv/lib/python3.12/site-packages (from anyio<5,>=3.5.0->groq) (3.11)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyproject_hooks in ./venv/lib/python3.12/site-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./venv/lib/python3.12/site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.2)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in ./venv/lib/python3.12/site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in ./venv/lib/python3.12/site-packages (from google-auth<3.0.0,>=2.45.0->google-auth[requests]<3.0.0,>=2.45.0->google-genai) (6.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in ./venv/lib/python3.12/site-packages (from google-auth<3.0.0,>=2.45.0->google-auth[requests]<3.0.0,>=2.45.0->google-genai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in ./venv/lib/python3.12/site-packages (from google-auth<3.0.0,>=2.45.0->google-auth[requests]<3.0.0,>=2.45.0->google-genai) (4.9.1)\n",
      "Requirement already satisfied: certifi in ./venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->groq) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in ./venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in ./venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
      "Requirement already satisfied: filelock in ./venv/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./venv/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.12.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./venv/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: decorator>=4.3.2 in ./venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers>=1.0.0 in ./venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.18.1 in ./venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel) (0.19.2)\n",
      "Requirement already satisfied: pexpect>4.3 in ./venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in ./venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.11.0 in ./venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel) (2.19.2)\n",
      "Requirement already satisfied: stack_data>=0.6.0 in ./venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel) (0.6.3)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in ./venv/lib/python3.12/site-packages (from jsonschema>=4.19.0->chromadb) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in ./venv/lib/python3.12/site-packages (from jsonschema>=4.19.0->chromadb) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in ./venv/lib/python3.12/site-packages (from jsonschema>=4.19.0->chromadb) (0.30.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.12/site-packages (from jupyter-client>=8.0.0->ipykernel) (2.9.0.post0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in ./venv/lib/python3.12/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel) (4.5.1)\n",
      "Requirement already satisfied: six>=1.9.0 in ./venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in ./venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (1.9.0)\n",
      "Requirement already satisfied: requests-oauthlib in ./venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: urllib3<2.4.0,>=1.24.2 in ./venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (2.3.0)\n",
      "Requirement already satisfied: durationpy>=0.7 in ./venv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
      "Requirement already satisfied: langchain-text-splitters<2.0.0,>=1.1.0 in ./venv/lib/python3.12/site-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in ./venv/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (1.33)\n",
      "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in ./venv/lib/python3.12/site-packages (from langchain-core<2.0.0,>=1.2.1->langchain) (0.12.0)\n",
      "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in ./venv/lib/python3.12/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.0.1)\n",
      "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in ./venv/lib/python3.12/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.5)\n",
      "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in ./venv/lib/python3.12/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.3.1)\n",
      "Requirement already satisfied: xxhash>=3.5.0 in ./venv/lib/python3.12/site-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in ./venv/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in ./venv/lib/python3.12/site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.25.0)\n",
      "Requirement already satisfied: coloredlogs in ./venv/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in ./venv/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb) (25.12.19)\n",
      "Requirement already satisfied: protobuf in ./venv/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n",
      "Requirement already satisfied: sympy in ./venv/lib/python3.12/site-packages (from onnxruntime>=1.14.1->chromadb) (1.14.0)\n",
      "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in ./venv/lib/python3.12/site-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.1)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.57 in ./venv/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.72.0)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.39.1 in ./venv/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.39.1)\n",
      "Requirement already satisfied: opentelemetry-proto==1.39.1 in ./venv/lib/python3.12/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.39.1)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.60b1 in ./venv/lib/python3.12/site-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.60b1)\n",
      "Requirement already satisfied: backoff>=1.10.0 in ./venv/lib/python3.12/site-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./venv/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in ./venv/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->groq) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in ./venv/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->groq) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./venv/lib/python3.12/site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.4.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./venv/lib/python3.12/site-packages (from rich>=10.11.0->chromadb) (4.0.0)\n",
      "Requirement already satisfied: greenlet>=1 in ./venv/lib/python3.12/site-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.3.0)\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (80.9.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in ./venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in ./venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in ./venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in ./venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in ./venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.90)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in ./venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in ./venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in ./venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in ./venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in ./venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in ./venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in ./venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in ./venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in ./venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in ./venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in ./venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in ./venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.5.1 in ./venv/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.5.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./venv/lib/python3.12/site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./venv/lib/python3.12/site-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
      "Requirement already satisfied: click>=8.0.0 in ./venv/lib/python3.12/site-packages (from typer>=0.9.0->chromadb) (8.3.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in ./venv/lib/python3.12/site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in ./venv/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.7.1)\n",
      "Requirement already satisfied: uvloop>=0.15.1 in ./venv/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.22.1)\n",
      "Requirement already satisfied: watchfiles>=0.13 in ./venv/lib/python3.12/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.1)\n",
      "Requirement already satisfied: joblib>=1.3.0 in ./venv/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (1.5.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.2.0 in ./venv/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: zipp>=3.20 in ./venv/lib/python3.12/site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in ./venv/lib/python3.12/site-packages (from jedi>=0.18.1->ipython>=7.23.1->ipykernel) (0.8.5)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./venv/lib/python3.12/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.1->langchain) (3.0.0)\n",
      "Requirement already satisfied: ormsgpack>=1.12.0 in ./venv/lib/python3.12/site-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.12.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./venv/lib/python3.12/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in ./venv/lib/python3.12/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel) (0.2.14)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in ./venv/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.45.0->google-auth[requests]<3.0.0,>=2.45.0->google-genai) (0.6.1)\n",
      "Requirement already satisfied: executing>=1.2.0 in ./venv/lib/python3.12/site-packages (from stack_data>=0.6.0->ipython>=7.23.1->ipykernel) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in ./venv/lib/python3.12/site-packages (from stack_data>=0.6.0->ipython>=7.23.1->ipykernel) (3.0.1)\n",
      "Requirement already satisfied: pure-eval in ./venv/lib/python3.12/site-packages (from stack_data>=0.6.0->ipython>=7.23.1->ipykernel) (0.2.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./venv/lib/python3.12/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./venv/lib/python3.12/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in ./venv/lib/python3.12/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./venv/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in ./venv/lib/python3.12/site-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb) (3.3.1)\n"
     ]
    }
   ],
   "source": [
    "#create a virtual environment\n",
    "!python3 -m venv venv\n",
    "\n",
    "#install packages with pip to virtual environment\n",
    "!./venv/bin/pip install groq langchain langchain-community sentence-transformers chromadb pypdf python-dotenv ipykernel google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "aa9a7eff",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installed kernelspec ai_agent_env in /home/yyg/.local/share/jupyter/kernels/ai_agent_env\r\n"
     ]
    }
   ],
   "source": [
    "#add virtual environment as a kernel to jupyter\n",
    "!./venv/bin/python -m ipykernel install --user --name=ai_agent_env --display-name \"AI Agent (Venv)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "03739672",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any\n",
    "from dataclasses import dataclass, asdict\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "#RAG imports\n",
    "from groq import Groq\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "#gemini import\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "31549026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clients initialized\n"
     ]
    }
   ],
   "source": [
    "client = Groq(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
    "gemini_client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "\n",
    "print(\"Clients initialized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b03af68e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching PDF...\n",
      "PDF loaded successfully - 70 pages\n",
      "PDF chunked - 685 chunks created\n",
      "Vector database created successfully\n"
     ]
    }
   ],
   "source": [
    "print(\"Fetching PDF...\")\n",
    "loader = PyPDFLoader(\"kasko_kitapcigi_01_2024.pdf\") \n",
    "pages = loader.load()\n",
    "print(f\"PDF loaded successfully - {len(pages)} pages\")\n",
    "\n",
    "#split into chunks with better parameters\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500, \n",
    "    chunk_overlap=50,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    ")\n",
    "chunks = text_splitter.split_documents(pages)\n",
    "print(f\"PDF chunked - {len(chunks)} chunks created\")\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    ")\n",
    "\n",
    "vector_db = Chroma.from_documents(chunks, embeddings)\n",
    "print(\"Vector database created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f6368119",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insurance_lookup_tool(query):\n",
    "    print(f\"\\nSearching: '{query}'\")\n",
    "    \n",
    "    #retrieve more results for better coverage\n",
    "    docs = vector_db.similarity_search(query, k=7)\n",
    "    \n",
    "    if not docs:\n",
    "        return \"No relevant information found in document.\"\n",
    "    \n",
    "    #combine relevant chunks\n",
    "    content = \"\"\n",
    "    for i, doc in enumerate(docs):\n",
    "        page_num = doc.metadata.get('page', 'N/A')\n",
    "        content += f\"\\n[Source {i+1} - Page {page_num}]\\n{doc.page_content}\\n\"\n",
    "    \n",
    "    #debug preview\n",
    "    preview = content[:400].replace('\\n', ' ')\n",
    "    print(f\"Found {len(docs)} results. Preview: {preview}...\")\n",
    "    \n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2a4d5d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are an expert AI assistant specializing in Anadolu Insurance (kasko) documents.\n",
    "Your task is to answer user questions by using the 'insurance_lookup_tool' to search the document.\n",
    "\n",
    "WORKFLOW (ReAct Pattern):\n",
    "1. Thought: Analyze the question and determine which Turkish keywords to search for\n",
    "2. Action: insurance_lookup_tool(\"short keywords\") - Write query inside parentheses\n",
    "3. Observation: Receive document search results\n",
    "4. Repeat Thought/Action: Search with different terms if needed\n",
    "5. Final Answer: Provide professional answer based ONLY on document information\n",
    "\n",
    "CRITICAL RULES:\n",
    "1. Use SHORT and CONCISE keywords (e.g., \"fuel filling\", \"engine damage\")\n",
    "2. Prefer Turkish terms since document is in Turkish\n",
    "3. NEVER fabricate limits/rates not in document\n",
    "4. If info not found, clearly state so - don't guess\n",
    "5. Action format: insurance_lookup_tool(\"search term\")\n",
    "\n",
    "EXAMPLE:\n",
    "Thought: User is asking about wrong fuel filling damage. I'll search \"wrong fuel\" and \"engine damage\".\n",
    "Action: insurance_lookup_tool(\"yanlış yakıt dolumu motor\")\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4936637e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_action(response_text):\n",
    "    patterns = [\n",
    "        r'Action:\\s*insurance_lookup_tool\\([\"\\'](.+?)[\"\\']\\)',  # Action: insurance_lookup_tool(\"query\")\n",
    "        r'Action:\\s*insurance_lookup_tool\\((.+?)\\)',             # Action: insurance_lookup_tool(query)\n",
    "        r'Action:\\s*[\"\\'](.+?)[\"\\']',                            # Action: \"query\"\n",
    "        r'Action:\\s*(.+?)(?:\\n|$)',                              # Action: query\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, response_text, re.IGNORECASE)\n",
    "        if match:\n",
    "            query = match.group(1).strip().strip('\"\\'')\n",
    "            return query\n",
    "    \n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ddd4bfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_insurance_agent(\n",
    "    user_question,\n",
    "    model=\"groq\",\n",
    "    max_steps=6,\n",
    "    verbose=True\n",
    "):\n",
    "    \n",
    "    if model == \"gemini\":\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": SYSTEM_PROMPT + \"\\n\\n\" + user_question}\n",
    "        ]\n",
    "    else:\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": user_question}\n",
    "        ]\n",
    "\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"USER QUESTION: {user_question}\")\n",
    "        print(f\"{'='*60}\")\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        #get response from model\n",
    "        if model == \"groq\":\n",
    "            completion = client.chat.completions.create(\n",
    "                model=\"llama-3.3-70b-versatile\",\n",
    "                messages=messages,\n",
    "                temperature=0.1,\n",
    "                max_tokens=1000,\n",
    "                stop=[\"Observation:\", \"Observation :\"]\n",
    "            )\n",
    "            agent_response = completion.choices[0].message.content.strip()\n",
    "            \n",
    "        elif model == \"gemini\":\n",
    "            contents = []\n",
    "            for msg in messages:\n",
    "                role = \"user\" if msg[\"role\"] == \"user\" else \"model\"\n",
    "                contents.append(types.Content(role=role, parts=[types.Part(text=msg[\"content\"])]))\n",
    "\n",
    "            response = gemini_client.models.generate_content(\n",
    "                model=\"gemini-1.5-pro\",\n",
    "                contents=contents,\n",
    "                config={\n",
    "                    'temperature': 0.1,\n",
    "                    'max_output_tokens': 1000,\n",
    "                    'stop_sequences': [\"Observation:\", \"Observation :\"]\n",
    "                }\n",
    "            )\n",
    "            agent_response = response.text.strip()\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model: {model}\")\n",
    "\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n{'─'*60}\")\n",
    "            print(f\"STEP {step + 1}:\")\n",
    "            print(f\"{'─'*60}\")\n",
    "            print(agent_response)\n",
    "        \n",
    "        #add agent response to memory\n",
    "        messages.append({\"role\": \"assistant\", \"content\": agent_response})\n",
    "        \n",
    "        #check for Final Answer\n",
    "        if \"Final Answer:\" in agent_response:\n",
    "            final_answer = agent_response.split(\"Final Answer:\")[-1].strip()\n",
    "            if verbose:\n",
    "                print(f\"\\n{'='*60}\")\n",
    "                print(f\"RESULT:\")\n",
    "                print(f\"{'='*60}\")\n",
    "                print(final_answer)\n",
    "            return final_answer\n",
    "        \n",
    "        #check for Action\n",
    "        if \"Action:\" in agent_response:\n",
    "            query = parse_action(agent_response)\n",
    "            \n",
    "            if query:\n",
    "                try:\n",
    "                    #perform search\n",
    "                    observation = insurance_lookup_tool(query)\n",
    "                    \n",
    "                    #add observation to system (truncated to prevent context overflow)\n",
    "                    obs_message = f\"Observation: Document search completed.\\n{observation[:2000]}\"\n",
    "                    messages.append({\"role\": \"assistant\", \"content\": obs_message})\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    error_msg = f\"Observation: Error during search: {str(e)}\"\n",
    "                    messages.append({\"role\": \"assistant\", \"content\": error_msg})\n",
    "                    print(f\" Error: {e}\")\n",
    "            else:\n",
    "                #could not parse Action\n",
    "                messages.append({\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": \"Please write Action in correct format: insurance_lookup_tool(\\\"search term\\\")\"\n",
    "                })\n",
    "        else:\n",
    "            #no Final Answer or Action, nudge the model\n",
    "            messages.append({\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Please continue with Thought/Action loop or provide Final Answer.\"\n",
    "            })\n",
    "    \n",
    "    return f\"Couldn't reach definitive answer in {max_steps} steps. Please make question more specific.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f2dcad11",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "USER QUESTION: Aracıma yanlış yakıt dolumu yapıldı, motor yandı. Hasarım ödenir mi? Limitim nedir?\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01kdntyrm5fd7vw3dwktknqrjd` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99871, Requested 293. Please try again in 2m21.696s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRateLimitError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[89]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#test question\u001b[39;00m\n\u001b[32m      2\u001b[39m question = \u001b[33m\"\u001b[39m\u001b[33mAracıma yanlış yakıt dolumu yapıldı, motor yandı. Hasarım ödenir mi? Limitim nedir?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mrun_insurance_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[88]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mrun_insurance_agent\u001b[39m\u001b[34m(user_question, model, max_steps, verbose)\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_steps):\n\u001b[32m     25\u001b[39m     \u001b[38;5;66;03m#get response from model\u001b[39;00m\n\u001b[32m     26\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model == \u001b[33m\"\u001b[39m\u001b[33mgroq\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m         completion = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mllama-3.3-70b-versatile\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mObservation:\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mObservation :\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m         agent_response = completion.choices[\u001b[32m0\u001b[39m].message.content.strip()\n\u001b[32m     36\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m model == \u001b[33m\"\u001b[39m\u001b[33mgemini\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/ai_agent/venv/lib/python3.12/site-packages/groq/resources/chat/completions.py:461\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, citation_options, compound_custom, disable_tool_validation, documents, exclude_domains, frequency_penalty, function_call, functions, include_domains, include_reasoning, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, n, parallel_tool_calls, presence_penalty, reasoning_effort, reasoning_format, response_format, search_settings, seed, service_tier, stop, store, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    241\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    242\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    243\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m    300\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m    301\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m    302\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    303\u001b[39m \u001b[33;03m    Creates a model response for the given chat conversation.\u001b[39;00m\n\u001b[32m    304\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    459\u001b[39m \u001b[33;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[32m    460\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m461\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/openai/v1/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    464\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcitation_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcitation_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    468\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompound_custom\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompound_custom\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    469\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdisable_tool_validation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable_tool_validation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    470\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdocuments\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    471\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mexclude_domains\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude_domains\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    472\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    473\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    475\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minclude_domains\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_domains\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    476\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minclude_reasoning\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_reasoning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    478\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msearch_settings\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msearch_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/ai_agent/venv/lib/python3.12/site-packages/groq/_base_client.py:1242\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1228\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1229\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1230\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1237\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1238\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1239\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1240\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1241\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1242\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/ai_agent/venv/lib/python3.12/site-packages/groq/_base_client.py:1044\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1041\u001b[39m             err.response.read()\n\u001b[32m   1043\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1044\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1046\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1048\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mRateLimitError\u001b[39m: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.3-70b-versatile` in organization `org_01kdntyrm5fd7vw3dwktknqrjd` service tier `on_demand` on tokens per day (TPD): Limit 100000, Used 99871, Requested 293. Please try again in 2m21.696s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}"
     ]
    }
   ],
   "source": [
    "#test question\n",
    "question = \"Aracıma yanlış yakıt dolumu yapıldı, motor yandı. Hasarım ödenir mi? Limitim nedir?\"\n",
    "print(run_insurance_agent(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007d211f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class BenchmarkQuestion:\n",
    "    id: int\n",
    "    question: str\n",
    "    ground_truth: str\n",
    "    category: str  # \"kapsam\", \"limit\", \"prosedür\", \"istisna\"\n",
    "    difficulty: str  # \"easy\", \"medium\", \"hard\"\n",
    "    keywords: List[str]\n",
    "\n",
    "@dataclass\n",
    "class EvaluationResult:\n",
    "    question_id: int\n",
    "    model_name: str\n",
    "    question: str\n",
    "    response: str\n",
    "    ground_truth: str\n",
    "    correctness_score: float\n",
    "    hallucination_detected: bool\n",
    "    retrieval_quality: float\n",
    "    completeness_score: float\n",
    "    response_time: float\n",
    "    keywords_found: List[str]\n",
    "    keywords_missing: List[str]\n",
    "    evaluation_notes: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cad9d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BenchmarkDataset:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.questions = self._create_questions()\n",
    "    \n",
    "    def _create_questions(self) -> List[BenchmarkQuestion]:\n",
    "        q_list = []\n",
    "        \n",
    "        # --- 1. EASY(1-15) ---\n",
    "        q_list.extend([\n",
    "            BenchmarkQuestion(1, \"Yanlış yakıt dolumu sonucu motor hasarı ödenir mi?\", \"Evet, belirli bir limite kadar (poliçede yazar) teminat altındadır.\", \"kapsam\", \"easy\", [\"yanlış yakıt\", \"motor\", \"hasar\"]),\n",
    "            BenchmarkQuestion(2, \"Cam kırılması kasko kapsamında mıdır?\", \"Evet, cam hasarları teminat altındadır.\", \"kapsam\", \"easy\", [\"cam\", \"kırılma\"]),\n",
    "            BenchmarkQuestion(3, \"Mini onarım hizmeti neleri kapsar?\", \"Küçük ölçekli kaporta, boya, döşeme ve cam hasarlarını kapsar.\", \"hizmet\", \"easy\", [\"mini onarım\", \"kapsam\"]),\n",
    "            BenchmarkQuestion(4, \"Aracım çalınırsa kasko ödeme yapar mı?\", \"Evet, hırsızlık ana teminattır.\", \"kapsam\", \"easy\", [\"çalınma\", \"hırsızlık\"]),\n",
    "            BenchmarkQuestion(5, \"Yedek anahtar kaybı kasko kapsamında mı?\", \"Evet, kilit sisteminin değiştirilmesi teminata dahildir.\", \"kapsam\", \"easy\", [\"anahtar\", \"kayıp\"]),\n",
    "            BenchmarkQuestion(6, \"İkame araç hizmeti kaç gündür?\", \"Poliçe türüne göre (genelde 7-15 gün) değişir.\", \"limit\", \"easy\", [\"ikame araç\", \"gün\"]),\n",
    "            BenchmarkQuestion(7, \"Lastik patlaması kasko kapsamında mıdır?\", \"Sadece patlama değil, kaza ile birlikte hasar oluşursa ödenir.\", \"istisna\", \"easy\", [\"lastik\", \"patlama\"]),\n",
    "            BenchmarkQuestion(8, \"Ferdi kaza teminatı vefat durumunu kapsar mı?\", \"Evet, poliçedeki limitler dahilinde kapsar.\", \"kapsam\", \"easy\", [\"ferdi kaza\", \"vefat\"]),\n",
    "            BenchmarkQuestion(9, \"Deprem hasarları kaskoya dahil mi?\", \"Evet, kasko genel şartlarına ek olarak teminata dahildir.\", \"kapsam\", \"easy\", [\"deprem\", \"doğal afet\"]),\n",
    "            BenchmarkQuestion(10, \"Sel hasarı sonucu araç pert olursa ne olur?\", \"Rayiç değer üzerinden ödeme yapılır.\", \"kapsam\", \"easy\", [\"sel\", \"pert\"]),\n",
    "            BenchmarkQuestion(11, \"Yangın teminatı sadece kaza anındakileri mi kapsar?\", \"Hayır, park halindeki yanmaları da kapsar.\", \"kapsam\", \"easy\", [\"yangın\", \"yanma\"]),\n",
    "            BenchmarkQuestion(12, \"Kemirgen hayvanların verdiği zararlar ödenir mi?\", \"Evet, fare vb. hayvanların kablo kemirmesi teminat altındadır.\", \"kapsam\", \"easy\", [\"kemirgen\", \"hayvan\"]),\n",
    "            BenchmarkQuestion(13, \"Kasko poliçesi satış durumunda ne olur?\", \"Poliçe kendiliğinden sonlanır (münfesih olur).\", \"prosedür\", \"easy\", [\"satış\", \"devir\"]),\n",
    "            BenchmarkQuestion(14, \"Hukuksal koruma teminatı limiti nedir?\", \"Poliçede belirtilen avukatlık ve mahkeme gideri limitidir.\", \"limit\", \"easy\", [\"hukuksal koruma\", \"limit\"]),\n",
    "            BenchmarkQuestion(15, \"Orijinal cam değişimi her zaman mümkün müdür?\", \"Poliçe türüne (Muafiyetli/Muafiyetsiz) göre değişir.\", \"prosedür\", \"easy\", [\"orijinal cam\", \"servis\"])\n",
    "        ])\n",
    "\n",
    "        # --- 2. MEDIUM (16-35) ---\n",
    "        q_list.extend([\n",
    "            BenchmarkQuestion(16, \"Anahtar ile çalınma (evden çalınma) durumu kapsamda mı?\", \"Kilitli olmayan yerden veya zorlama olmadan çalınma şartlarına bağlıdır.\", \"kapsam\", \"medium\", [\"anahtar\", \"evden çalınma\"]),\n",
    "            BenchmarkQuestion(17, \"Yurt dışında kaza yaparsam kaskom geçerli olur mu?\", \"Sadece Türkiye sınırları içinde geçerlidir, ek teminat gerekir.\", \"istisna\", \"medium\", [\"yurt dışı\", \"sınır\"]),\n",
    "            BenchmarkQuestion(18, \"Aracın içindeki kişisel eşyaların çalınması ödenir mi?\", \"Belirli bir limite kadar ve kilitli araç şartıyla ödenir.\", \"limit\", \"medium\", [\"kişisel eşya\", \"çalınma\"]),\n",
    "            BenchmarkQuestion(19, \"Hasar bildirim süresi kaç iş günüdür?\", \"Genel şartlara göre 5 iş günüdür.\", \"prosedür\", \"medium\", [\"bildirim\", \"süre\", \"ihbar\"]),\n",
    "            BenchmarkQuestion(20, \"Özel servis ile yetkili servis farkı hasar ödemesini etkiler mi?\", \"Poliçede belirtilen servis ağı dışında kesinti yapılabilir.\", \"prosedür\", \"medium\", [\"özel servis\", \"yetkili servis\"]),\n",
    "            BenchmarkQuestion(21, \"Eşdeğer parça kullanımı ne zaman başlar?\", \"Araç yaş ve parça türüne göre yasal mevzuata göre uygulanır.\", \"prosedür\", \"medium\", [\"eşdeğer parça\", \"yan sanayi\"]),\n",
    "            BenchmarkQuestion(22, \"LPG takılan aracın hasarı ödenir mi?\", \"Ruhsata işli olması ve sigortacıya bildirilmiş olması gerekir.\", \"istisna\", \"medium\", [\"lpg\", \"tüp\"]),\n",
    "            BenchmarkQuestion(23, \"Kusurlu olduğum kazada karşı tarafın hasarı nasıl ödenir?\", \"İMM (İhtiyari Mali Mesuliyet) limitleri dahilinde ödenir.\", \"limit\", \"medium\", [\"imm\", \"karşı taraf\"]),\n",
    "            BenchmarkQuestion(24, \"Hatalı park yüzünden araç çekilirken hasar görürse ne olur?\", \"Çekici Anadolu Asistans ise teminat altındadır.\", \"kapsam\", \"medium\", [\"hatalı park\", \"çekici\"]),\n",
    "            BenchmarkQuestion(25, \"Valeye teslim edilen aracın çalınması ödenir mi?\", \"İşletmenin sorumluluğundadır, kasko genellikle rücu eder veya reddeder.\", \"istisna\", \"medium\", [\"vale\", \"teslim\"]),\n",
    "            BenchmarkQuestion(26, \"Terör olayları hasarları hangi şartla ödenir?\", \"Ek teminat olarak poliçeye eklenmiş olmalıdır.\", \"kapsam\", \"medium\", [\"terör\", \"eylem\"]),\n",
    "            BenchmarkQuestion(27, \"Arıza nedeniyle yolda kalan araca hangi hizmet verilir?\", \"Çekici ve asistans hizmetleri verilir ancak parça ödenmez.\", \"hizmet\", \"medium\", [\"arıza\", \"yolda kalma\"]),\n",
    "            BenchmarkQuestion(28, \"Hasarsızlık indirimi cam hasarında bozulur mu?\", \"Yılda bir kez cam hasarı indirimi bozmaz (poliçeye göre).\", \"prosedür\", \"medium\", [\"hasarsızlık\", \"bozulma\"]),\n",
    "            BenchmarkQuestion(29, \"Araçta sigara yanığı oluşması kasko kapsamında mı?\", \"Genellikle ek teminatla veya 'dahili hasar' kapsamında değerlendirilir.\", \"kapsam\", \"medium\", [\"sigara yanığı\", \"döşeme\"]),\n",
    "            BenchmarkQuestion(30, \"Kıymet kazanma tenzili nedir?\", \"Eski parçanın yerine yenisi takıldığında oluşan değer artış düşüşüdür.\", \"prosedür\", \"medium\", [\"kıymet kazanma\", \"eskime\"]),\n",
    "            BenchmarkQuestion(31, \"Aracın pert olması için hasar oranı ne olmalıdır?\", \"Genellikle %50-%70 arası hasar ve onarım maliyeti bakılır.\", \"limit\", \"medium\", [\"pert\", \"hasar oranı\"]),\n",
    "            BenchmarkQuestion(32, \"Ehliyetsiz araç kullanımı hasar ödemesini nasıl etkiler?\", \"Kesinlikle tazminat ödenmez, kasko dışıdır.\", \"istisna\", \"medium\", [\"ehliyetsiz\", \"red\"]),\n",
    "            BenchmarkQuestion(33, \"Poliçe vadesi bitmeden iptal edilirse prim iadesi olur mu?\", \"Gün esasına göre iade yapılır.\", \"prosedür\", \"medium\", [\"iptal\", \"prim iadesi\"]),\n",
    "            BenchmarkQuestion(34, \"Radyo-teyp çalınması teminata dahil midir?\", \"Orijinal ise dahildir, sonradan takıldıysa bildirilmelidir.\", \"kapsam\", \"medium\", [\"radyo\", \"teyp\"]),\n",
    "            BenchmarkQuestion(35, \"Kıvılcım sıçraması sonucu oluşan hasarlar yangın sayılır mı?\", \"Evet, yangın teminatı kapsamında değerlendirilir.\", \"kapsam\", \"medium\", [\"kıvılcım\", \"yangın\"])\n",
    "        ])\n",
    "\n",
    "        # --- 3. HARD (36-40) ---\n",
    "        q_list.extend([\n",
    "            BenchmarkQuestion(36, \"Sel sırasında motoru çalıştırmak hasarın reddine yol açar mı?\", \"Brüt kusur ve hasarı ağırlaştırma gerekçesiyle red sebebi olabilir.\", \"istisna\", \"hard\", [\"sel\", \"motor çalıştırma\", \"su çekme\"]),\n",
    "            BenchmarkQuestion(37, \"6 ay önce yapılan onarımın tekrarlaması durumunda garanti var mı?\", \"Anlaşmalı servislerde onarım garantisi poliçede belirtilir.\", \"istisna\", \"hard\", [\"tekrar hasar\", \"garanti\"]),\n",
    "            BenchmarkQuestion(38, \"Alkollü araç kullanımında rücu şartları nelerdir?\", \"Zorunlu trafik sigortası ve kasko genel şartlarına göre belirlenir.\", \"istisna\", \"hard\", [\"alkol\", \"rücu\", \"promil\"]),\n",
    "            BenchmarkQuestion(39, \"İstihap haddi (aşırı yük) aşımı hasarı nasıl etkiler?\", \"Hasar ile illiyet bağı varsa tazminat ödenmeyebilir.\", \"istisna\", \"hard\", [\"istihap haddi\", \"aşırı yük\"]),\n",
    "            BenchmarkQuestion(40, \"Aracın ikinci el değer kaybı kasko tarafından karşılanır mı?\", \"Kasko sadece hasarı onarır, değer kaybı genellikle kapsam dışıdır.\", \"istisna\", \"hard\", [\"değer kaybı\", \"ikinci el\"])\n",
    "        ])\n",
    "\n",
    "        # --- 4. OUT-OF-SCOPE (41-50) ---\n",
    "        q_list.extend([\n",
    "            BenchmarkQuestion(41, \"2025 yılı zorunlu trafik sigortası tavan fiyatları nedir?\", \"Dokümanda bu bilgi bulunmamaktadır (Sadece Kasko içerir).\", \"out_of_scope\", \"hard\", [\"trafik sigortası\", \"fiyat\"]),\n",
    "            BenchmarkQuestion(42, \"BMW F30 motor yağı kaç kilometrede değişir?\", \"Dokümanda teknik bakım kılavuzu bulunmamaktadır.\", \"out_of_scope\", \"hard\", [\"motor yağı\", \"bakım\"]),\n",
    "            BenchmarkQuestion(43, \"Anadolu Sigorta hisseleri bugün borsada kaç TL?\", \"Güncel finansal veriler dokümanda yoktur.\", \"out_of_scope\", \"hard\", [\"borsa\", \"hisse\"]),\n",
    "            BenchmarkQuestion(44, \"Konut sigortası kapsamında su tesisatı arızası ödenir mi?\", \"Bu kasko dokümanıdır, konut sigortasını kapsamaz.\", \"out_of_scope\", \"hard\", [\"konut sigortası\", \"tesisat\"]),\n",
    "            BenchmarkQuestion(45, \"En iyi kasko şirketi hangisidir?\", \"Doküman objektif poliçe şartlarını içerir, kıyaslama yapmaz.\", \"out_of_scope\", \"hard\", [\"en iyi\", \"kıyas\"]),\n",
    "            BenchmarkQuestion(46, \"Apple CarPlay bağlantı hatası nasıl düzeltilir?\", \"Teknik yazılım desteği kasko kapsamında değildir.\", \"out_of_scope\", \"hard\", [\"carplay\", \"bağlantı\"]),\n",
    "            BenchmarkQuestion(47, \"Almanya plakalı araca Türkiye'den kasko yapılır mı?\", \"Doküman tescil mevzuatı içermez.\", \"out_of_scope\", \"hard\", [\"almanya\", \"plaka\"]),\n",
    "            BenchmarkQuestion(48, \"Sigorta Genel Müdürü kimdir?\", \"Personel bilgisi dokümanda yer almamaktadır.\", \"out_of_scope\", \"hard\", [\"genel müdür\", \"kimdir\"]),\n",
    "            BenchmarkQuestion(49, \"MTV ödemesi kredi kartına taksit yapılır mı?\", \"Vergi ödeme detayları sigorta poliçesinde yer almaz.\", \"out_of_scope\", \"hard\", [\"mtv\", \"taksit\"]),\n",
    "            BenchmarkQuestion(50, \"Kaza sonrası karşı tarafa açılacak dava için avukat önerir misin?\", \"Doküman avukat tavsiyesi içermez.\", \"out_of_scope\", \"hard\", [\"avukat önerisi\", \"dava\"])\n",
    "        ])\n",
    "        \n",
    "        return q_list\n",
    "    \n",
    "    def get_by_category(self, category: str) -> List[BenchmarkQuestion]:\n",
    "        return [q for q in self.questions if q.category == category]\n",
    "    \n",
    "    def get_by_difficulty(self, difficulty: str) -> List[BenchmarkQuestion]:\n",
    "        return [q for q in self.questions if q.difficulty == difficulty]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638e0927",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluator:\n",
    "    def evaluate_response(self, question: BenchmarkQuestion, response: str, \n",
    "                         response_time: float) -> EvaluationResult:\n",
    "        \n",
    "        keywords_found = [kw for kw in question.keywords \n",
    "                         if kw.lower() in response.lower()]\n",
    "        keywords_missing = [kw for kw in question.keywords \n",
    "                           if kw.lower() not in response.lower()]\n",
    "        \n",
    "        completeness = len(keywords_found) / len(question.keywords) if question.keywords else 0\n",
    "        \n",
    "        hallucination_indicators = [\n",
    "            \"approximately\", \"around\", \"roughly\", \"I think\", \"probably\",\n",
    "            \"tahminimce\", \"yaklaşık\", \"sanırım\", \"olabilir\"\n",
    "        ]\n",
    "        hallucination_detected = any(ind in response.lower() for ind in hallucination_indicators)\n",
    "        \n",
    "        correctness_score = 0.7\n",
    "        \n",
    "        has_numbers = any(char.isdigit() for char in response)\n",
    "        has_specific_terms = any(term in response.lower() for term in \n",
    "                                [\"tl\", \"₺\", \"limit\", \"madde\", \"bent\"])\n",
    "        vague_terms = [\"genel olarak\", \"genellikle\", \"çoğunlukla\", \"bazı durumlarda\"]\n",
    "        has_vague = any(term in response.lower() for term in vague_terms)\n",
    "        \n",
    "        retrieval_quality = 0.5\n",
    "        if has_numbers: retrieval_quality += 0.2\n",
    "        if has_specific_terms: retrieval_quality += 0.2\n",
    "        if has_vague: retrieval_quality -= 0.3\n",
    "        retrieval_quality = max(0.0, min(1.0, retrieval_quality))\n",
    "        \n",
    "        return EvaluationResult(\n",
    "            question_id=question.id,\n",
    "            model_name=\"\",\n",
    "            question=question.question,\n",
    "            response=response,\n",
    "            ground_truth=question.ground_truth,\n",
    "            correctness_score=correctness_score,\n",
    "            hallucination_detected=hallucination_detected,\n",
    "            retrieval_quality=retrieval_quality,\n",
    "            completeness_score=completeness,\n",
    "            response_time=response_time,\n",
    "            keywords_found=keywords_found,\n",
    "            keywords_missing=keywords_missing,\n",
    "            evaluation_notes=\"\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac079432",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BenchmarkRunner:\n",
    "    \n",
    "    def __init__(self, dataset: BenchmarkDataset, evaluator: ModelEvaluator):\n",
    "        self.dataset = dataset\n",
    "        self.evaluator = evaluator\n",
    "        self.results: List[EvaluationResult] = []\n",
    "    \n",
    "    def run_benchmark(self, model_name: str, model_type: str, limit: int = None) -> List[EvaluationResult]:\n",
    "        \n",
    "        questions = self.dataset.questions[:limit] if limit else self.dataset.questions\n",
    "        results = []\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"Running Benchmark for: {model_name}\")\n",
    "        print(f\"Total Questions: {len(questions)}\")\n",
    "        print(f\"{'='*70}\\n\")\n",
    "        \n",
    "        for i, question in enumerate(questions, 1):\n",
    "            print(f\"[{i}/{len(questions)}] Processing Q{question.id}...\")\n",
    "            \n",
    "            start_time = time.time()\n",
    "            \n",
    "            try:\n",
    "                response = run_insurance_agent(\n",
    "                    question.question, \n",
    "                    verbose=False, \n",
    "                    model=model_type\n",
    "                )\n",
    "                response_time = time.time() - start_time\n",
    "                \n",
    "                eval_result = self.evaluator.evaluate_response(\n",
    "                    question, response, response_time\n",
    "                )\n",
    "                eval_result.model_name = model_name\n",
    "                \n",
    "                results.append(eval_result)\n",
    "                \n",
    "                print(f\"Completed in {response_time:.2f}s | Score: {eval_result.correctness_score:.2f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "                continue\n",
    "            \n",
    "            time.sleep(0.5)\n",
    "        \n",
    "        self.results.extend(results)\n",
    "        return results\n",
    "    \n",
    "    def generate_report(self, results: List[EvaluationResult]) -> Dict[str, Any]:\n",
    "        \n",
    "        if not results:\n",
    "            return {\n",
    "                \"model\": \"UNKNOWN\",\n",
    "                \"total_questions\": 0,\n",
    "                \"metrics\": {\n",
    "                    \"avg_correctness\": 0.0,\n",
    "                    \"avg_completeness\": 0.0,\n",
    "                    \"avg_retrieval_quality\": 0.0,\n",
    "                    \"avg_response_time\": 0.0,\n",
    "                    \"hallucination_rate\": 0.0\n",
    "                },\n",
    "                \"by_difficulty\": {},\n",
    "                \"detailed_results\": []\n",
    "            }\n",
    "        \n",
    "        model_name = results[0].model_name\n",
    "        \n",
    "        avg_correctness = sum(r.correctness_score for r in results) / len(results)\n",
    "        avg_completeness = sum(r.completeness_score for r in results) / len(results)\n",
    "        avg_retrieval = sum(r.retrieval_quality for r in results) / len(results)\n",
    "        avg_time = sum(r.response_time for r in results) / len(results)\n",
    "        hallucination_rate = sum(1 for r in results if r.hallucination_detected) / len(results)\n",
    "        \n",
    "        easy_results = [r for r in results if any(\n",
    "            q.id == r.question_id and q.difficulty == \"easy\" \n",
    "            for q in self.dataset.questions\n",
    "        )]\n",
    "        medium_results = [r for r in results if any(\n",
    "            q.id == r.question_id and q.difficulty == \"medium\" \n",
    "            for q in self.dataset.questions\n",
    "        )]\n",
    "        hard_results = [r for r in results if any(\n",
    "            q.id == r.question_id and q.difficulty == \"hard\" \n",
    "            for q in self.dataset.questions\n",
    "        )]\n",
    "        \n",
    "        report = {\n",
    "            \"model\": model_name,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"total_questions\": len(results),\n",
    "            \"metrics\": {\n",
    "                \"avg_correctness\": round(avg_correctness, 3),\n",
    "                \"avg_completeness\": round(avg_completeness, 3),\n",
    "                \"avg_retrieval_quality\": round(avg_retrieval, 3),\n",
    "                \"avg_response_time\": round(avg_time, 2),\n",
    "                \"hallucination_rate\": round(hallucination_rate, 3)\n",
    "            },\n",
    "            \"by_difficulty\": {\n",
    "                \"easy\": {\n",
    "                    \"count\": len(easy_results),\n",
    "                    \"avg_score\": round(sum(r.correctness_score for r in easy_results) / len(easy_results), 3) if easy_results else 0\n",
    "                },\n",
    "                \"medium\": {\n",
    "                    \"count\": len(medium_results),\n",
    "                    \"avg_score\": round(sum(r.correctness_score for r in medium_results) / len(medium_results), 3) if medium_results else 0\n",
    "                },\n",
    "                \"hard\": {\n",
    "                    \"count\": len(hard_results),\n",
    "                    \"avg_score\": round(sum(r.correctness_score for r in hard_results) / len(hard_results), 3) if hard_results else 0\n",
    "                }\n",
    "            },\n",
    "            \"detailed_results\": [asdict(r) for r in results]\n",
    "        }\n",
    "        \n",
    "        return report\n",
    "    \n",
    "    def compare_models(self, reports: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        \n",
    "        comparison = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"models\": [r[\"model\"] for r in reports],\n",
    "            \"comparison\": {}\n",
    "        }\n",
    "        \n",
    "        metrics = [\"avg_correctness\", \"avg_completeness\", \"avg_retrieval_quality\", \n",
    "                  \"avg_response_time\", \"hallucination_rate\"]\n",
    "        \n",
    "        for metric in metrics:\n",
    "            comparison[\"comparison\"][metric] = {\n",
    "                r[\"model\"]: r[\"metrics\"][metric] \n",
    "                for r in reports\n",
    "            }\n",
    "            \n",
    "            if metric == \"avg_response_time\" or metric == \"hallucination_rate\":\n",
    "                best_model = min(reports, key=lambda x: x[\"metrics\"][metric])[\"model\"]\n",
    "            else:\n",
    "                best_model = max(reports, key=lambda x: x[\"metrics\"][metric])[\"model\"]\n",
    "            \n",
    "            comparison[\"comparison\"][metric][\"best\"] = best_model\n",
    "        \n",
    "        if len(reports) == 2:\n",
    "            base_report = reports[0]\n",
    "            comp_report = reports[1]\n",
    "            \n",
    "            comparison[\"percentage_differences\"] = {}\n",
    "            for metric in metrics:\n",
    "                base_val = base_report[\"metrics\"][metric]\n",
    "                comp_val = comp_report[\"metrics\"][metric]\n",
    "                \n",
    "                if base_val != 0:\n",
    "                    diff = ((comp_val - base_val) / base_val) * 100\n",
    "                    comparison[\"percentage_differences\"][metric] = {\n",
    "                        \"difference\": round(diff, 2),\n",
    "                        \"interpretation\": f\"{comp_report['model']} is {abs(diff):.1f}% {'better' if diff > 0 else 'worse'}\"\n",
    "                    }\n",
    "        \n",
    "        return comparison\n",
    "    \n",
    "    def save_results(self, filepath: str):\n",
    "        \n",
    "        models = list(set(r.model_name for r in self.results))\n",
    "        reports = []\n",
    "        \n",
    "        for model in models:\n",
    "            model_results = [r for r in self.results if r.model_name == model]\n",
    "            report = self.generate_report(model_results)\n",
    "            reports.append(report)\n",
    "        \n",
    "        comparison = self.compare_models(reports) if len(reports) > 1 else None\n",
    "        \n",
    "        output = {\n",
    "            \"reports\": reports,\n",
    "            \"comparison\": comparison\n",
    "        }\n",
    "        \n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            json.dump(output, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"\\nResults saved to: {filepath}\")\n",
    "    \n",
    "    def print_summary(self, report: Dict[str, Any]):\n",
    "        if report.get('model') == \"UNKNOWN\" or 'metrics' not in report:\n",
    "            print(\"\\n!!! HATA: Model hiçbir soruyu yanıtlayamadı, rapor oluşturulamadı. !!!\")\n",
    "            return\n",
    "        \n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"BENCHMARK RESULTS: {report['model']}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"Total Questions: {report['total_questions']}\")\n",
    "        print(f\"\\nMETRICS:\")\n",
    "        print(f\"  Correctness:        {report['metrics']['avg_correctness']:.1%}\")\n",
    "        print(f\"  Completeness:       {report['metrics']['avg_completeness']:.1%}\")\n",
    "        print(f\"  Retrieval Quality:  {report['metrics']['avg_retrieval_quality']:.1%}\")\n",
    "        print(f\"  Avg Response Time:  {report['metrics']['avg_response_time']:.2f}s\")\n",
    "        print(f\"  Hallucination Rate: {report['metrics']['hallucination_rate']:.1%}\")\n",
    "        \n",
    "        print(f\"\\nBY DIFFICULTY:\")\n",
    "        for diff in [\"easy\", \"medium\", \"hard\"]:\n",
    "            data = report['by_difficulty'][diff]\n",
    "            if data['count'] > 0:\n",
    "                print(f\"  {diff.capitalize():8} ({data['count']:2} questions): {data['avg_score']:.1%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29e1fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_questions = [\n",
    "    \"Aracıma yanlış yakıt dolumu yapıldı, motor yandı. Hasarım ödenir mi? Limitim nedir?\",\n",
    "    \"Kasko sigortam cam kırılmalarını karşılar mı?\",\n",
    "    \"Mini onarım hizmeti nedir?\",\n",
    "]\n",
    "\n",
    "for i, question in enumerate(test_questions):\n",
    "    print(f\"\\n\\n{'#'*60}\")\n",
    "    print(f\"TEST {i+1}\")\n",
    "    print(f\"{'#'*60}\")\n",
    "    result = run_insurance_agent(question, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1da612c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"### Manuel Sorgu Testi ###\")\n",
    "test_results = vector_db.similarity_search(\"yakıt\", k=3)\n",
    "for i, res in enumerate(test_results):\n",
    "    print(f\"Result {i+1} (Page {res.metadata.get('page')}):\")\n",
    "    print(res.page_content)\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd6809f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#initialize benchmark system\n",
    "dataset = BenchmarkDataset()\n",
    "evaluator = ModelEvaluator()\n",
    "runner = BenchmarkRunner(dataset, evaluator)\n",
    "\n",
    "#test with Groq Llama 3.3\n",
    "groq_results = runner.run_benchmark(\n",
    "    model_name=\"Groq Llama-3.3-70B\",\n",
    "    model_type=\"groq\",\n",
    "    limit=5\n",
    ")\n",
    "groq_report = runner.generate_report(groq_results)\n",
    "runner.print_summary(groq_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364b48f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#test with Gemini Pro (first 5 questions)\n",
    "gemini_results = runner.run_benchmark(\n",
    "    model_name=\"Google Gemini-1.5-Pro\",\n",
    "    model_type=\"gemini\",\n",
    "    limit=5\n",
    ")\n",
    "gemini_report = runner.generate_report(gemini_results)\n",
    "runner.print_summary(gemini_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a36cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate comparison report\n",
    "comparison = runner.compare_models([groq_report, gemini_report])\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"n\" + \"=\"*70)\n",
    "print(json.dumps(comparison, indent=2, ensure_ascii=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32451dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save all results to JSON file\n",
    "runner.save_results(\"benchmark_results.json\")\n",
    "print(\"\\nBenchmark completed! Check benchmark_results.json for details.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cb5f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = input(\"Sigorta ile ilgili sorunuzu yazın: \")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "answer = run_insurance_agent(user_input, verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI Agent (Venv)",
   "language": "python",
   "name": "ai_agent_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
